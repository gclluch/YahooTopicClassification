{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and create training/validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load cleaned data and create training/test/validation split\n",
    "\n",
    "data_train = pd.read_csv('data/yahoo_train_clean.csv')\n",
    "data_test = pd.read_csv('data/yahoo_test_clean.csv')\n",
    "data_train = data_train.sample(frac=.3, random_state=42)\n",
    "\n",
    "X = data_train['text']\n",
    "Y = data_train['class']\n",
    "\n",
    "X_test_text = data_test['text']\n",
    "y_test = data_test['class']\n",
    "\n",
    "X_train_text, X_valid_text, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "max_vocab = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary bag-of-words representation  (unigrams and bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def BBoW(X_train, X_valid, X_test) : \n",
    "    bow_transformer = CountVectorizer(max_features=max_vocab, ngram_range=(1,2), binary=True ).fit(X_train.values.astype('U'))\n",
    "    X_train = bow_transformer.transform(X_train.values.astype('U'))\n",
    "    X_valid = bow_transformer.transform(X_valid.values.astype('U'))\n",
    "    X_test = bow_transformer.transform(X_test.values.astype('U'))\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf representation (unigrams and bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(X_train, X_valid, X_test) : \n",
    "    tfidf_transformer = TfidfVectorizer(max_features=max_vocab, ngram_range=(1,2)).fit(X_train.values.astype('U'))\n",
    "    X_train = tfidf_transformer.transform(X_train.values.astype('U'))\n",
    "    X_valid = tfidf_transformer.transform(X_valid.values.astype('U'))\n",
    "    X_test = tfidf_transformer.transform(X_test.values.astype('U'))\n",
    "    return X_train, X_valid, X_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create splits for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "\n",
    "def cv_split(X_train, X_valid, y_train, y_valid) : \n",
    "    my_test_fold = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        my_test_fold.append(-1)\n",
    "    for i in range(X_valid.shape[0]):\n",
    "        my_test_fold.append(0)\n",
    "\n",
    "    fold = PredefinedSplit(test_fold=my_test_fold)\n",
    "    Y_cv = np.append(y_train, y_valid)\n",
    "    X_cv = sp.vstack((X_train , X_valid)) \n",
    "    return X_cv, Y_cv, fold\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#5-fold cross-validation\n",
    "def MNB_cv(parameters, X, T, fold) : \n",
    "    n_folds = 5\n",
    "    mnb = MultinomialNB(fit_prior=True)\n",
    "    mnb_cv = GridSearchCV(mnb, parameters, cv=fold, scoring=\"accuracy\", refit=False)\n",
    "    mnb_cv.fit(X, T)\n",
    "    scores = mnb_cv.cv_results_['mean_test_score']\n",
    "    params = mnb_cv.cv_results_['params']\n",
    "#     print('scores:',scores)\n",
    "#    print('params:',params)\n",
    "    return mnb_cv.best_params_\n",
    "\n",
    "#search for alpha value\n",
    "def MNB_get_hyperparameters(values, scale, X_cv_split, Y_cv_split, fold) : \n",
    "    prevBest = 1000\n",
    "    while (True) : \n",
    "        param_grid = {'alpha': values}\n",
    "        bestParams = MNB_cv(param_grid, X_cv_split, Y_cv_split, fold)\n",
    "        curBest = bestParams['alpha']\n",
    "        if (abs(curBest - prevBest ) > .00001) : \n",
    "            inc = curBest/10\n",
    "            lb = curBest - inc*scale\n",
    "            ub = curBest + inc*scale\n",
    "            values = np.arange(lb, ub, inc)\n",
    "            prevBest = curBest\n",
    "        else : return curBest\n",
    "\n",
    "\n",
    "def MNB_predict(alpha, X_train, X_test, y_train, y_test) :\n",
    "    nb = MultinomialNB(alpha=alpha, fit_prior=True)\n",
    "    nb.fit(X_train, y_train)\n",
    "    #print('best alpha: ', alphaVal)\n",
    "   \n",
    "    #predict on train set\n",
    "    y_hat_train = nb.predict(X_train)\n",
    "    print('\\t Training accuracy: \\t', accuracy_score(y_train, y_hat_train)  )\n",
    "\n",
    "    #predict on test set\n",
    "    y_hat_test = nb.predict(X_test)\n",
    "    print('\\t Test accuracy: \\t', accuracy_score(y_test, y_hat_test)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Linear SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#5-fold cross-validation\n",
    "def linSVM_cv(parameters, X, T, fold) : \n",
    "    n_folds = 5\n",
    "    svc = LinearSVC(multi_class='ovr', max_iter = 2000, dual=False)\n",
    "    svc_cv = GridSearchCV(svc, parameters, cv=fold, scoring=\"accuracy\", refit=False)\n",
    "    svc_cv.fit(X, T)\n",
    "    scores = svc_cv.cv_results_['mean_test_score']\n",
    "    params = svc_cv.cv_results_['params']\n",
    "#    print('scores:',scores)\n",
    "#    print('params:',params)\n",
    "    return svc_cv.best_params_\n",
    "\n",
    "#search for C value\n",
    "def SVM_get_hyperparameters(values, scale, X_cv_split, Y_cv_split, fold) : \n",
    "    prevBest = 1000\n",
    "    while (True) : \n",
    "        param_grid = {'C': values}\n",
    "        bestParams = linSVM_cv(param_grid, X_cv_split, Y_cv_split, fold)\n",
    "        curBest = bestParams['C']\n",
    "        if (abs(curBest - prevBest ) > .0001) : \n",
    "            inc = curBest/10\n",
    "            lb = curBest - inc*scale\n",
    "            ub = curBest + inc*scale\n",
    "            values = np.arange(lb, ub, inc)\n",
    "            prevBest = curBest\n",
    "        else : return curBest\n",
    "\n",
    "def SVM_predict(C, X_train, X_test, y_train, y_test) :\n",
    "    \n",
    "    svm = LinearSVC(C=C, multi_class='ovr')\n",
    "    svm.fit(X_train, y_train)\n",
    "    #print('best C: ', cVal)\n",
    "\n",
    "    #predict on train set\n",
    "    y_hat_train = svm.predict(X_train)\n",
    "    print('\\t Training accuracy: \\t', accuracy_score(y_train, y_hat_train)  )\n",
    "\n",
    "    #predict on test set\n",
    "    y_hat_test = svm.predict(X_test)\n",
    "    print('\\t Test accuracy: \\t', accuracy_score(y_test, y_hat_test)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#5-fold cross-validation\n",
    "def LogReg_cv(parameters, X, T, fold) : \n",
    "    n_folds = 5\n",
    "    logreg = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter = 4000, dual=False)\n",
    "    logreg_cv = GridSearchCV(logreg, parameters, cv=fold, scoring=\"accuracy\", refit=False)\n",
    "    logreg_cv.fit(X, T) \n",
    "    scores = logreg_cv.cv_results_['mean_test_score']\n",
    "    params = logreg_cv.cv_results_['params']\n",
    "#    print('scores:',scores)\n",
    "#    print('params:',params)\n",
    "    return logreg_cv.best_params_\n",
    "\n",
    "#search for C value\n",
    "def LogReg_get_hyperparameters(values, scale, X_cv_split, Y_cv_split, fold) : \n",
    "    prevBest = 1000\n",
    "    while (True) : \n",
    "        param_grid = {'C': values}\n",
    "        bestParams = LogReg_cv(param_grid, X_cv_split, Y_cv_split, fold)\n",
    "        curBest = bestParams['C']\n",
    "        if (abs(curBest - prevBest ) > .0001) : \n",
    "            inc = curBest/10\n",
    "            lb = curBest - inc*scale\n",
    "            ub = curBest + inc*scale\n",
    "            values = np.arange(lb, ub, inc)\n",
    "            prevBest = curBest\n",
    "        else : return curBest\n",
    "\n",
    "        \n",
    "def LogReg_predict(C, X_train, X_test, y_train, y_test) :     \n",
    "    logreg = LogisticRegression(C=C, multi_class='ovr', solver='lbfgs',  max_iter = 4000, dual=False)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    #print('best C: ', cVal)\n",
    "\n",
    "    #predict on train set\n",
    "    y_hat_train = logreg.predict(X_train)\n",
    "    print('\\t Training accuracy: \\t', accuracy_score(y_train, y_hat_train)  )\n",
    "\n",
    "    #predict on test set\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "    print('\\t Test accuracy: \\t', accuracy_score(y_test, y_hat_test)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBoW - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = BBoW(X_train_text, X_valid_text, X_test_text)\n",
    "X_cv_split, Y_cv_split, fold = cv_split(X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "values = [ 0.0001, 0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: \t 0.6632232142857143\n",
      "\t Test accuracy: \t 0.6565833333333333\n"
     ]
    }
   ],
   "source": [
    "alpha = MNB_get_hyperparameters(values, 5, X_cv_split, Y_cv_split, fold) \n",
    "\n",
    "MNB_predict(alpha, X_train, X_test, y_train, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: \t 0.7096875\n",
      "\t Test accuracy: \t 0.68335\n"
     ]
    }
   ],
   "source": [
    "C = SVM_get_hyperparameters(values, 5, X_cv_split, Y_cv_split, fold) \n",
    "\n",
    "SVM_predict(C, X_train, X_test, y_train, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: \t 0.7091160714285715\n",
      "\t Test accuracy: \t 0.6806\n"
     ]
    }
   ],
   "source": [
    "C = LogReg_get_hyperparameters(values, 5, X_cv_split, Y_cv_split, fold) \n",
    "\n",
    "LogReg_predict(C, X_train, X_test, y_train, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = tfidf(X_train_text, X_valid_text, X_test_text)\n",
    "X_cv_split, Y_cv_split, fold = cv_split(X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "values = [ 0.0001, 0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: \t 0.6802470238095238\n",
      "\t Test accuracy: \t 0.6735\n"
     ]
    }
   ],
   "source": [
    "alpha = MNB_get_hyperparameters(values, 5, X_cv_split, Y_cv_split, fold) \n",
    "\n",
    "MNB_predict(alpha, X_train, X_test, y_train, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: \t 0.7234642857142857\n",
      "\t Test accuracy: \t 0.6954\n"
     ]
    }
   ],
   "source": [
    "C = SVM_get_hyperparameters(values, 5, X_cv_split, Y_cv_split, fold) \n",
    "\n",
    "SVM_predict(C, X_train, X_test, y_train, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training accuracy: \t 0.7158214285714286\n",
      "\t Test accuracy: \t 0.69335\n"
     ]
    }
   ],
   "source": [
    "C = LogReg_get_hyperparameters(values, 5, X_cv_split, Y_cv_split, fold) \n",
    "\n",
    "LogReg_predict(C, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
